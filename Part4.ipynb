{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Compute scores for negative triples\n",
    "def compute_scores(neg_triples, model):\n",
    "    return np.array([model.score(triple) for triple in neg_triples])\n",
    "\n",
    "# Generate self-adversarial weights\n",
    "def compute_weights(scores, alpha=1.0):\n",
    "    exp_scores = np.exp(alpha * scores)\n",
    "    return exp_scores / exp_scores.sum()\n",
    "\n",
    "# Sample hard negatives based on weights\n",
    "def sample_negatives(neg_triples, weights, num_samples):\n",
    "    indices = np.random.choice(len(neg_triples), size=num_samples, p=weights)\n",
    "    return [neg_triples[i] for i in indices]\n",
    "\n",
    "# Training step with adversarial negatives\n",
    "def train_step(model, pos_triples, neg_triples, alpha=1.0):\n",
    "    neg_scores = compute_scores(neg_triples, model)\n",
    "    weights = compute_weights(neg_scores, alpha)\n",
    "    hard_negatives = sample_negatives(neg_triples, weights, len(pos_triples))\n",
    "\n",
    "    # Train the model with positive and hard negative samples\n",
    "    loss = model.train(pos_triples, hard_negatives)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Does self-adversarial sampling improve metric scores compared to standard negative sampling?**\n",
    "\n",
    "Yes, it often improves MRR and Hits@K because it reduces the gap between training and test distribution by making the model robust to difficult cases. The model learns finer-grained decision boundaries, improving its ability to distinguish between true and false links.\n",
    "\n",
    "**Why does self-adversarial sampling generate “harder” negatives, and what is its impact on training?**\n",
    "\n",
    "Harder Negatives:\n",
    "They are chosen because they have high similarity scores (according to the model) with true triples, making them more challenging. This forces the model to better understand subtle differences between correct and incorrect triples.\n",
    "\n",
    "Impact on Training:\n",
    "It slows convergence slightly because the model must focus on finer-grained adjustments.\n",
    "However, the overall quality of embeddings and generalization improves, leading to better performance metrics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
